Rerun all hyperparameter tuning experiments for reproducibility (~23 hrs), delete unused scripts and functions. Note that there was a bug in a pad_normdata.yaml previously used for the preprocessing experiments (padding, normalization): the statistics calculated for normdata.yaml were also being used for pad_normdata.yaml. I recalculated the statistics for pad_normdata.yaml and used those to train the respective models instead, so the results for the pad_normdata replicates are slightly different. Training output for all other models was reproduced, and the key results didn't change. Main takeaways are that preprocessing, weight decay, and upsampling do not change performance (precision and recall) noticeabley compared to the base model. A higher learning rate degrades performance, and a lower learning rate does not improve it. There was also a minor bug in how metrics for pad-*.pt predictions were being calculated, which are fixed here (did not affect interpretation of results).